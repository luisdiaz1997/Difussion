{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Simple DDPM on MNIST - Just an MLP!\n",
    "\n",
    "This notebook uses the **simplest possible** diffusion model:\n",
    "- Just a 3-layer MLP (Multi-Layer Perceptron)\n",
    "- ~500K parameters instead of millions\n",
    "- Trains in ~10 minutes\n",
    "- **Actually works on MNIST!**\n",
    "\n",
    "No U-Net, no convolutions, no attention - just the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from realistica import NoiseScheduler, SimpleMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "config = {\n",
    "    'image_size': 28,  # Keep MNIST at original size\n",
    "    'batch_size': 128,\n",
    "    'num_epochs': 30,  # More epochs since model is simple\n",
    "    'learning_rate': 1e-3,  # Can use higher LR for MLP\n",
    "    'num_timesteps': 1000,\n",
    "}\n",
    "\n",
    "os.makedirs('outputs/mnist_simple/samples', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "\n",
    "# Show examples\n",
    "sample_batch, _ = next(iter(train_loader))\n",
    "grid = make_grid(sample_batch[:64], nrow=8, normalize=True, value_range=(-1, 1))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Training Images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Simple MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = NoiseScheduler(\n",
    "    num_timesteps=config['num_timesteps'],\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02,\n",
    "    schedule_type='linear',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Simple MLP - just 3 hidden layers!\n",
    "model = SimpleMLP(\n",
    "    image_size=28,\n",
    "    hidden_dim=512,\n",
    "    time_emb_dim=128\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "losses = []\n",
    "model.train()\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "    \n",
    "    for images, _ in progress_bar:\n",
    "        images = images.to(device)\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Sample timesteps\n",
    "        t = noise_scheduler.sample_timesteps(batch_size)\n",
    "        \n",
    "        # Add noise\n",
    "        noisy_images, noise = noise_scheduler.add_noise(images, t)\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = model(noisy_images, t)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(predicted_noise, noise)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, alpha=0.4)\n",
    "plt.plot(np.convolve(losses, np.ones(50)/50, mode='valid'), linewidth=2)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend(['Raw', 'Smoothed'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(num_samples=64):\n",
    "    model.eval()\n",
    "    \n",
    "    # Start from pure noise\n",
    "    x = torch.randn(num_samples, 1, 28, 28, device=device)\n",
    "    \n",
    "    # Denoise step by step\n",
    "    for t in tqdm(reversed(range(noise_scheduler.num_timesteps)), desc='Sampling'):\n",
    "        t_batch = torch.tensor([t] * num_samples, device=device)\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = model(x, t_batch)\n",
    "        \n",
    "        # Get scheduler values\n",
    "        alpha = noise_scheduler.alphas[t]\n",
    "        alpha_cumprod = noise_scheduler.alphas_cumprod[t]\n",
    "        beta = noise_scheduler.betas[t]\n",
    "        \n",
    "        # Add noise if not last step\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "        else:\n",
    "            noise = torch.zeros_like(x)\n",
    "        \n",
    "        # Denoise\n",
    "        x = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_cumprod)) * predicted_noise)\n",
    "        x = x + torch.sqrt(beta) * noise\n",
    "    \n",
    "    model.train()\n",
    "    return x\n",
    "\n",
    "# Generate\n",
    "samples = sample(64)\n",
    "\n",
    "# Visualize\n",
    "grid = make_grid(samples, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Generated MNIST Digits (Simple MLP)')\n",
    "plt.savefig('outputs/mnist_simple/samples/final_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Denoising Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_with_steps(num_samples=8, steps_to_show=10):\n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.randn(num_samples, 1, 28, 28, device=device)\n",
    "    \n",
    "    timesteps = list(reversed(range(noise_scheduler.num_timesteps)))\n",
    "    step_interval = len(timesteps) // steps_to_show\n",
    "    saved_steps = []\n",
    "    \n",
    "    for i, t in enumerate(tqdm(timesteps, desc='Denoising')):\n",
    "        if i % step_interval == 0 or i == len(timesteps) - 1:\n",
    "            saved_steps.append(x.cpu().clone())\n",
    "        \n",
    "        t_batch = torch.tensor([t] * num_samples, device=device)\n",
    "        predicted_noise = model(x, t_batch)\n",
    "        \n",
    "        alpha = noise_scheduler.alphas[t]\n",
    "        alpha_cumprod = noise_scheduler.alphas_cumprod[t]\n",
    "        beta = noise_scheduler.betas[t]\n",
    "        \n",
    "        noise = torch.randn_like(x) if t > 0 else torch.zeros_like(x)\n",
    "        \n",
    "        x = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_cumprod)) * predicted_noise)\n",
    "        x = x + torch.sqrt(beta) * noise\n",
    "    \n",
    "    model.train()\n",
    "    return saved_steps\n",
    "\n",
    "steps = sample_with_steps(8, 10)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(8, len(steps), figsize=(15, 10))\n",
    "for i in range(8):\n",
    "    for j, step_imgs in enumerate(steps):\n",
    "        axes[i, j].imshow(step_imgs[i, 0].numpy(), cmap='gray', vmin=-1, vmax=1)\n",
    "        axes[i, j].axis('off')\n",
    "        if i == 0:\n",
    "            axes[i, j].set_title(f'Step {j}')\n",
    "\n",
    "plt.suptitle('Denoising Process: Noise â†’ Digit', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/mnist_simple/samples/denoising_process.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**This simple MLP proves:**\n",
    "1. You DON'T need U-Nets for simple datasets\n",
    "2. A basic 3-layer MLP with ~500K params works fine on MNIST\n",
    "3. Much faster to train and easier to understand\n",
    "4. Good for learning and experimentation\n",
    "\n",
    "**When to use what:**\n",
    "- **SimpleMLP**: MNIST, simple datasets, fast experiments\n",
    "- **SimpleCNN**: Better quality on MNIST, still fast\n",
    "- **U-Net**: Real images (CelebA, ImageNet, etc.)\n",
    "\n",
    "**If you're getting maze patterns:**\n",
    "- Train for more epochs (30-50)\n",
    "- Check your data normalization (should be [-1, 1])\n",
    "- Lower learning rate if loss explodes\n",
    "- SimpleMLP is less prone to artifacts than U-Net!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
